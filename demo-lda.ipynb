{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topic Modeling using LDA\n",
    "\n",
    "This notebook demonstrates how to implement topic modeling using Latent Dirichlet Allocation (LDA) on news feed. \n",
    "\n",
    "LDA is a generative model which is commonly used in topic modeling. In LDA, we assume that each document is generated by a mixtures of latent topics, and each topic has probabilities of generating various words. The use of Dirichlet prior in the model is based on the intuition that a document can only cover a small set of topics and topics use only small set of words frequently.\n",
    "\n",
    "In this example, a news feed can be viewed as a mixture of various latent topics. Our goal is to find word counts of top words appeared in each news feed, and predict topics which are most likely to generate this news feed according to the word count vector. We first preprocess the news feed by removing stop words and then represent the text into bag of words matrix. LDA takes bag of words matrix as inputs and output a document to topic matrix and a word to topic matrix. These two matrices when multiplied reproduce the bag of words matrix with the lowest error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim import corpora, models, similarities\n",
    "from utils import clean_text, build_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Preprocessing\n",
    "\n",
    "First, let us load and clean the text data. The dataset comprises chinese news feed which fall into 3 topics, namely '梁振英', '美國大選', '足球' ('Hong Kong Chief Executive', 'US President Election', 'Soccer').\n",
    "\n",
    "To train LDA model using Gensim library, we have to tokenize the text into list of words. We also remove words that appear less than 100 times in all news feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/cd/k8wp_1kd24g6w3k6pxhs99_m0000gn/T/jieba.cache\n",
      "Loading model cost 0.779 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before text preprocessing:\n",
      "利物浦重賽擊敗乙組仔　英足盃過關 英格蘭足總盃第三圈今晨重賽，貴為英超勁旅的利物浦上場被乙組仔埃克斯特尷尬逼和，多獲一次機會的紅軍不敢再有差池。先有近期回勇的「威爾斯沙維」祖阿倫10分鐘開紀錄，加上兩個小將舒爾奧祖，及祖奧迪西拿下半場各入一球，以3比0擊敗對手，總算在主場挽 <p style=\"text-align: justify;\">英格蘭足總盃第三圈今晨重賽，貴為英超勁旅的利物浦上場被乙組仔埃克斯特尷尬逼和，多獲一次機會的紅軍不敢再有差池。先有近期回勇的「威爾斯沙維」祖阿倫10分鐘開紀錄，加上兩個小將舒爾奧祖，及祖奧迪西拿下半場各入一球，以3比0擊敗對手，總算在主場挽回面子，下一圈對手為韋斯咸。</p> <p style=\"text-align: justify;\">另一場英超球隊對壘，今季異軍突起的李斯特城戰至第三圈就宣告畢業。熱刺憑韓國前鋒孫興<U+615C>上半場遠射破網先開紀錄，換邊後此子助攻予中場查迪尼建功，令球隊以兩球輕取李斯特城，第四圈將面對英甲的高車士打。</p>\n",
      "\n",
      "After text preprocessing:\n",
      "利物浦 重賽 擊敗 乙組 仔英足 盃 過關   英格蘭足 總 盃 第三圈 今晨 重賽 貴為 英超 勁 旅 利物浦 上場 乙 組仔 埃克斯 特 尷尬 逼 多獲 一次 機會的紅 軍 不敢 差池 先有 近期 回勇 威爾斯沙維 祖阿倫 分鐘 開紀錄 加上 兩個 小將 舒爾奧祖 及祖奧 迪西 拿下 半場 各入 一球 以比擊敗 手 總算 主場 挽   英格蘭足 總 盃 第三圈 今晨 重賽 貴為 英超 勁 旅 利物浦 上場 乙 組仔 埃克斯 特 尷尬 逼 多獲 一次 機會的紅 軍 不敢 差池 先有 近期 回勇 威爾斯沙維 祖阿倫 分鐘 開紀錄 加上 兩個 小將 舒爾奧祖 及祖奧 迪西 拿下 半場 各入 一球 以比擊敗 手 總算 主場 挽回 面子 一圈 對手 韋斯咸   一場 英超球 隊 壘 今季異 軍 突起 李斯特 城戰 第三圈 宣告 畢業 熱刺 韓國 前鋒 孫興 上半 場遠射 破 網先 開紀錄 換邊 此子 助攻 予中場 查迪尼 建功 令球隊 兩球 輕取 李斯特 城 四圈 將面 英甲 高車士\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\"data/newsfeed.csv\",  header=0)\n",
    "X = df['text']\n",
    "y = df['tags']\n",
    "\n",
    "# text preprocessing\n",
    "stopwords = build_stopwords(filepath='data/stopwords.txt')\n",
    "X_cleaned = clean_text(X, stopwords)\n",
    "\n",
    "print('Before text preprocessing:')\n",
    "print(X[0])\n",
    "print('\\nAfter text preprocessing:')\n",
    "print(X_cleaned[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokenized text data:  3894\n",
      "['利物浦', '擊敗', '盃', '總', '盃', '今晨', '英超', '勁', '利物浦', '上場', '逼', '一次', '軍', '不敢', '近期', '分鐘', '開紀錄', '加上', '兩個', '小將', '半場', '一球', '手', '主場', '總', '盃', '今晨', '英超', '勁', '利物浦', '上場', '逼', '一次', '軍', '不敢', '近期', '分鐘', '開紀錄', '加上', '兩個', '小將', '半場', '一球', '手', '主場', '對手', '韋斯咸', '一場', '英超球', '隊', '軍', '李斯特', '熱刺', '韓國', '前鋒', '上半', '開紀錄', '助攻', '建功', '兩球', '李斯特', '城']\n"
     ]
    }
   ],
   "source": [
    "# text to list of words\n",
    "texts = []\n",
    "for text in X_cleaned:\n",
    "    texts.append(text.split())\n",
    "    \n",
    "# remove words that appear less than 100 times \n",
    "freq = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        freq[token] += 1\n",
    "texts = [[token for token in text if freq[token] > 100] for text in texts]\n",
    "\n",
    "print(\"Number of tokenized text data: \", len(texts))\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train LDA model\n",
    "\n",
    "LDA model in Gensim takes dictionary and corpus as inputs and output the topic-word distributions. We first convert the tokenized text into dictionary and corpus, and then train the LDA model with number of topics = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1888 unique tokens: ['一場', '一次', '一球', '上半', '上場']...)\n",
      "Dictionary size:  1888\n",
      "Corpus size:  3894\n"
     ]
    }
   ],
   "source": [
    "# text to dictionary\n",
    "dic = corpora.Dictionary(texts)\n",
    "print(dic)\n",
    "\n",
    "# text to corpus\n",
    "corpus = [dic.doc2bow(text) for text in texts]\n",
    "\n",
    "# save dictionary and corpus for future use\n",
    "dic.save('model/news.dict')\n",
    "gensim.corpora.MmCorpus.serialize('model/news.mm', corpus)\n",
    "\n",
    "# load corpus and dictionar if necessary\n",
    "# dic = gensim.corpora.Dictionary.load('model/news.dict')\n",
    "# corpus = gensim.corpora.MmCorpus('model/news.mm')\n",
    "\n",
    "print('Dictionary size: ', len(dic))\n",
    "print('Corpus size: ', len(corpus))\n",
    "\n",
    "# build LDA model\n",
    "lda = models.LdaModel(corpus, id2word=dic, num_topics=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate the LDA model\n",
    "\n",
    "To evaluate the trained LDA model, we can extract the topic-word distributions. Let us see the top 5 most contributing words for the 3 topics. Here are some of the observations.\n",
    "\n",
    "* Topic 0 contains words such as 'HK Chief Executive', 'Hong Kong', which is related to 'Hong Kong Chief Executive'\n",
    "* Topic 1 contains words such as 'soccer', 'Premier League', which is related to 'soccer'\n",
    "* Topic 2 contains words such as 'Donald Trump', 'Hillary', 'America', 'President' which is realted to 'US President Election'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.014*\"梁振英\" + 0.011*\"特首\" + 0.007*\"香港\" + 0.007*\"盃\" + 0.006*\"馬\"'),\n",
      " (1, '0.011*\"盃\" + 0.011*\"香港\" + 0.009*\"足球\" + 0.007*\"英超\" + 0.007*\"球員\"'),\n",
      " (2, '0.028*\"特朗普\" + 0.019*\"希拉里\" + 0.015*\"美國\" + 0.010*\"黨\" + 0.009*\"總統\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda.print_topics(num_words=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have ground truth for the news feed dataset. We can evaluate the LDA model performace using the accuracy score between the true topics and the predicted topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7668207498715973\n"
     ]
    }
   ],
   "source": [
    "# predict topic probability distribution for corpus\n",
    "corpus_lda = lda[corpus]\n",
    "\n",
    "# get the label (topic) with highest probability\n",
    "pred = list(map(lambda l: sorted(l, key=lambda label_prob: label_prob[1], reverse=True)[0][0], corpus_lda))\n",
    "\n",
    "# encode the label for ground truth\n",
    "class_dict = {0:'梁振英', 1:'足球', 2:'美國大選'}\n",
    "y_class = []\n",
    "for i in range(len(y)):\n",
    "    for j in range(len(class_dict)):\n",
    "        if y[i]==class_dict[j]: y_class.append(j)\n",
    "            \n",
    "# compute accuracy score\n",
    "print('Accuracy: ', accuracy_score(y_class, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
